<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Hadoop项目：从cdn日志统计直播流量 - Suncle&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Suncle" /><meta name="description" content="从在用的四家cdn的大量日志中，统计出每场直播的流量数据，包括国内流量和海外流量。
获取日志 目前已有的数据来源：四家cdn服务商。每个服务商都有自己不同的日志接口，不同的日志获取方式，可以把日志类型分为：
  网宿日志、帝联日志、阿里日志，腾讯日志
  直播日志、录播日志
  hls日志、rtmp日志、rtmpdist日志、hdl日志，不同协议日志的域名都不相同。
  各家厂商cdn日志的收集方法参见各自官网。获取到的日志示例文件名如下：
   cdn_code log_name     netcenter 2017-12-06-2300-2330_rtmp-wsz.qukanvideo.com.cn.log.gz   dnion hls-d.quklive.com_20180509_03_04.gz   alicdn play-a.quklive.com_2017_12_07_1100_1200.gz   qukan-&amp;gt;alicdn recordcdn-sz.qukanvideo.com_2017_12_06_1800_1900.gz   tencent 2017120607_hangzhouqukan.cdn.log.gz    可以从文件名判断属于日志所属的cdn代码和对应的协议。将cdn代码、播放类型代码、协议代码对应的关系直接存在字典中：
1 2 3 4  domain_protocol_dict = { &amp;#39;recordcdn.quklive.com&amp;#39;: (&amp;#39;qukan&amp;#39;, &amp;#39;record&amp;#39;, &amp;#39;hls&amp;#39;), ... }   通过日志名称匹配到域名，并取得对应的cdn代码、播放类型代码、协议代码，然后对具体的日志做不同的正则处理。
" /><meta name="keywords" content="计算机, 后端, Python, golang" />






<meta name="generator" content="Hugo 0.74.3 with theme even" />


<link rel="canonical" href="https://suncle.me/2018/05/16/Hadoop-Project-Live-Flow-Statistic/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Hadoop项目：从cdn日志统计直播流量" />
<meta property="og:description" content="从在用的四家cdn的大量日志中，统计出每场直播的流量数据，包括国内流量和海外流量。
获取日志
目前已有的数据来源：四家cdn服务商。每个服务商都有自己不同的日志接口，不同的日志获取方式，可以把日志类型分为：


网宿日志、帝联日志、阿里日志，腾讯日志


直播日志、录播日志


hls日志、rtmp日志、rtmpdist日志、hdl日志，不同协议日志的域名都不相同。


各家厂商cdn日志的收集方法参见各自官网。获取到的日志示例文件名如下：



cdn_code
log_name




netcenter
2017-12-06-2300-2330_rtmp-wsz.qukanvideo.com.cn.log.gz


dnion
hls-d.quklive.com_20180509_03_04.gz


alicdn
play-a.quklive.com_2017_12_07_1100_1200.gz


qukan-&gt;alicdn
recordcdn-sz.qukanvideo.com_2017_12_06_1800_1900.gz


tencent
2017120607_hangzhouqukan.cdn.log.gz



可以从文件名判断属于日志所属的cdn代码和对应的协议。将cdn代码、播放类型代码、协议代码对应的关系直接存在字典中：


1
2
3
4


domain_protocol_dict = {
    &#39;recordcdn.quklive.com&#39;: (&#39;qukan&#39;, &#39;record&#39;, &#39;hls&#39;),
    ...
}


通过日志名称匹配到域名，并取得对应的cdn代码、播放类型代码、协议代码，然后对具体的日志做不同的正则处理。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://suncle.me/2018/05/16/Hadoop-Project-Live-Flow-Statistic/" />
<meta property="article:published_time" content="2018-05-16T13:38:03+08:00" />
<meta property="article:modified_time" content="2018-05-16T13:38:03+08:00" />
<meta itemprop="name" content="Hadoop项目：从cdn日志统计直播流量">
<meta itemprop="description" content="从在用的四家cdn的大量日志中，统计出每场直播的流量数据，包括国内流量和海外流量。
获取日志
目前已有的数据来源：四家cdn服务商。每个服务商都有自己不同的日志接口，不同的日志获取方式，可以把日志类型分为：


网宿日志、帝联日志、阿里日志，腾讯日志


直播日志、录播日志


hls日志、rtmp日志、rtmpdist日志、hdl日志，不同协议日志的域名都不相同。


各家厂商cdn日志的收集方法参见各自官网。获取到的日志示例文件名如下：



cdn_code
log_name




netcenter
2017-12-06-2300-2330_rtmp-wsz.qukanvideo.com.cn.log.gz


dnion
hls-d.quklive.com_20180509_03_04.gz


alicdn
play-a.quklive.com_2017_12_07_1100_1200.gz


qukan-&gt;alicdn
recordcdn-sz.qukanvideo.com_2017_12_06_1800_1900.gz


tencent
2017120607_hangzhouqukan.cdn.log.gz



可以从文件名判断属于日志所属的cdn代码和对应的协议。将cdn代码、播放类型代码、协议代码对应的关系直接存在字典中：


1
2
3
4


domain_protocol_dict = {
    &#39;recordcdn.quklive.com&#39;: (&#39;qukan&#39;, &#39;record&#39;, &#39;hls&#39;),
    ...
}


通过日志名称匹配到域名，并取得对应的cdn代码、播放类型代码、协议代码，然后对具体的日志做不同的正则处理。">
<meta itemprop="datePublished" content="2018-05-16T13:38:03+08:00" />
<meta itemprop="dateModified" content="2018-05-16T13:38:03+08:00" />
<meta itemprop="wordCount" content="2707">



<meta itemprop="keywords" content="项目,Hadoop,统计," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hadoop项目：从cdn日志统计直播流量"/>
<meta name="twitter:description" content="从在用的四家cdn的大量日志中，统计出每场直播的流量数据，包括国内流量和海外流量。
获取日志
目前已有的数据来源：四家cdn服务商。每个服务商都有自己不同的日志接口，不同的日志获取方式，可以把日志类型分为：


网宿日志、帝联日志、阿里日志，腾讯日志


直播日志、录播日志


hls日志、rtmp日志、rtmpdist日志、hdl日志，不同协议日志的域名都不相同。


各家厂商cdn日志的收集方法参见各自官网。获取到的日志示例文件名如下：



cdn_code
log_name




netcenter
2017-12-06-2300-2330_rtmp-wsz.qukanvideo.com.cn.log.gz


dnion
hls-d.quklive.com_20180509_03_04.gz


alicdn
play-a.quklive.com_2017_12_07_1100_1200.gz


qukan-&gt;alicdn
recordcdn-sz.qukanvideo.com_2017_12_06_1800_1900.gz


tencent
2017120607_hangzhouqukan.cdn.log.gz



可以从文件名判断属于日志所属的cdn代码和对应的协议。将cdn代码、播放类型代码、协议代码对应的关系直接存在字典中：


1
2
3
4


domain_protocol_dict = {
    &#39;recordcdn.quklive.com&#39;: (&#39;qukan&#39;, &#39;record&#39;, &#39;hls&#39;),
    ...
}


通过日志名称匹配到域名，并取得对应的cdn代码、播放类型代码、协议代码，然后对具体的日志做不同的正则处理。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Suncle&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">首页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Suncle&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">首页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Hadoop项目：从cdn日志统计直播流量</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-05-16 </span>
        <div class="post-category">
            <a href="/categories/Hadoop/"> Hadoop </a>
            </div>
          <span class="more-meta"> 约 2707 字 </span>
          <span class="more-meta"> 预计阅读 6 分钟 </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#获取日志">获取日志</a></li>
    <li><a href="#日志存入hdfs">日志存入HDFS</a></li>
    <li><a href="#hadoop压缩日志">hadoop压缩日志</a></li>
    <li><a href="#mr程序">MR程序</a>
      <ul>
        <li><a href="#flow_statistic_mapperpy">flow_statistic_mapper.py</a></li>
        <li><a href="#flow_statistic_reducerpy">flow_statistic_reducer.py</a></li>
        <li><a href="#代码调试">代码调试</a></li>
        <li><a href="#mr调用">MR调用</a></li>
      </ul>
    </li>
    <li><a href="#流量数据导出到mysql">流量数据导出到Mysql</a></li>
    <li><a href="#hadoop-streaming错误排查">hadoop streaming错误排查</a></li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>从在用的四家cdn的大量日志中，统计出每场直播的流量数据，包括国内流量和海外流量。</p>
<h1 id="获取日志">获取日志</h1>
<p>目前已有的数据来源：四家cdn服务商。每个服务商都有自己不同的日志接口，不同的日志获取方式，可以把日志类型分为：</p>
<ul>
<li>
<p>网宿日志、帝联日志、阿里日志，腾讯日志</p>
</li>
<li>
<p>直播日志、录播日志</p>
</li>
<li>
<p>hls日志、rtmp日志、rtmpdist日志、hdl日志，不同协议日志的域名都不相同。</p>
</li>
</ul>
<p>各家厂商cdn日志的收集方法参见各自官网。获取到的日志示例文件名如下：</p>
<table>
<thead>
<tr>
<th>cdn_code</th>
<th>log_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>netcenter</td>
<td>2017-12-06-2300-2330_rtmp-wsz.qukanvideo.com.cn.log.gz</td>
</tr>
<tr>
<td>dnion</td>
<td>hls-d.quklive.com_20180509_03_04.gz</td>
</tr>
<tr>
<td>alicdn</td>
<td>play-a.quklive.com_2017_12_07_1100_1200.gz</td>
</tr>
<tr>
<td>qukan-&gt;alicdn</td>
<td>recordcdn-sz.qukanvideo.com_2017_12_06_1800_1900.gz</td>
</tr>
<tr>
<td>tencent</td>
<td>2017120607_hangzhouqukan.cdn.log.gz</td>
</tr>
</tbody>
</table>
<p>可以从文件名判断属于日志所属的cdn代码和对应的协议。将cdn代码、播放类型代码、协议代码对应的关系直接存在字典中：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">domain_protocol_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;recordcdn.quklive.com&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;qukan&#39;</span><span class="p">,</span> <span class="s1">&#39;record&#39;</span><span class="p">,</span> <span class="s1">&#39;hls&#39;</span><span class="p">),</span>
    <span class="o">...</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>通过日志名称匹配到域名，并取得对应的cdn代码、播放类型代码、协议代码，然后对具体的日志做不同的正则处理。</p>
<h1 id="日志存入hdfs">日志存入HDFS</h1>
<p><strong>直接使用hdfs命令拷贝到HDFS中</strong></p>
<p>通过日志下载程序调用接口下载到的日志可以使用以下命令直接拷贝到hdfs，拷贝成功一个日志，就删除对应的本地文件系统日志。示例命令如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">hdfs dfs -put /tmp/2018-05-09-0000-0030_rtmpdist-wsz.qukanvideo.com.cn.log.gz cdn_log
</code></pre></td></tr></table>
</div>
</div><p><strong>使用分布式日志收集系统flume导入到HDFS中</strong></p>
<p>对于用户访问日志的采集，更多的是使用flume，并且会将采集的数据保存到HDFS中 。虽然本次项目日志不需要采用此种方式，但是也可以作为一个手段。flume在分布式日志收集上比较类似于ELK中的logstash，可以对比学习下。最简单（单agent）的数据流模型如下：</p>
<p><img src="http://flume.apache.org/_images/UserGuide_image00.png" alt="flume data flow model"></p>
<p>具体使用方法参见：<a href="http://flume.apache.org/FlumeUserGuide.html">Flume 1.8.0 User Guide</a></p>
<h1 id="hadoop压缩日志">hadoop压缩日志</h1>
<p>各个cdn厂商提供的cdn日志都是gz格式的压缩日志，因此必须考虑对压缩日志的处理。Hadoop3 对于压缩格式是自动识别的。如果我们压缩的文件有相应压缩格式的扩展名（比如 lzo，gz，bzip2 等）。Hadoop 会根据压缩格式的扩展名自动选择相对应的解码器来解压数据，此过程完全是 Hadoop 自动处理，我们只需要确保输入的压缩文件有扩展名。因此这一步可以直接省略自行解压的操作。</p>
<p>但是需要注意在mapper环境变量中得到的输入文件的文件名是解压之前的文件名，也就是带压缩扩展名的。在hadoop3中可以通过以下变量验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="n">input_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;mapreduce_map_input_file&#39;</span><span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><p>需要稍微注意的地方有两点：</p>
<ol>
<li>input_file_path保存的是文件在hdfs上的完整路径。  比如：<code>hdfs://node-master:9000/user/hadoop/cdn_log/2018-05-09-0100-0130_rtmpdist-wsz.qukanvideo.com.cn.log.gz</code></li>
<li>老版本的api为<code>map_input_file</code>，在集群上尝试了老版本的api，代码会报错。</li>
</ol>
<h1 id="mr程序">MR程序</h1>
<p>具体代码参见Github：https://github.com/Flowsnow/hadoop-mapreduce-demo</p>
<p>需要先确定mapper和redecer中间的数据格式，需要考虑到shuffle。因为最终是要按照live_id分组进行统计，因此live_id作为key，中间数据如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">formatted_line = &#39;\t&#39;.join([live_id, datetime_str, ip, up_flow, down_flow])
</code></pre></td></tr></table>
</div>
</div><p>使用<code>'\t'</code>分隔，ip用于后续判断属于国内还是海外。</p>
<h2 id="flow_statistic_mapperpy">flow_statistic_mapper.py</h2>
<p>主要从各个cdn日志中筛选出有效的格式化数据，因此最多的操作就是对日志文件名和日志每一行进行正则匹配。</p>
<h2 id="flow_statistic_reducerpy">flow_statistic_reducer.py</h2>
<p>根据ip查询是国内流量还是海外流量，对每场直播进行统计。</p>
<h2 id="代码调试">代码调试</h2>
<p>使用linux管道、cat命令、sort命令综合使用进行调试，示例调试命令如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat /tmp/2018-05-09-0000-0030_rtmpdist-wsz.qukanvideo.com.cn.log <span class="p">|</span> python flow_statistic_mapper.py <span class="p">|</span> sort -t <span class="s1">$&#39;\t&#39;</span> -k1,1 <span class="p">|</span> python flow_statistic_reducer.py
</code></pre></td></tr></table>
</div>
</div><p>因为原始日志是压缩格式的，因此调试时可以先把日志解压然后调试，相对应的mapper中的输入文件名称也会有变化，需要注意。</p>
<h2 id="mr调用">MR调用</h2>
<p>命令和执行结果如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span><span class="lnt">90
</span><span class="lnt">91
</span><span class="lnt">92
</span><span class="lnt">93
</span><span class="lnt">94
</span><span class="lnt">95
</span><span class="lnt">96
</span><span class="lnt">97
</span><span class="lnt">98
</span><span class="lnt">99
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">hadoop@node-master:~/workspace/flow_statistic$ hadoop jar /usr/local/src/hadoop-3.1.0/share/hadoop/tools/lib/hadoop-streaming-3.1.0.jar -file flow_statistic_mapper.py -mapper &#39;python flow_statistic_mapper.py&#39; -file flow_statistic_reducer.py -reducer &#39;python flow_statistic_reducer.py&#39; -input all_cdn_logs/*.gz -output output-flow
2018-05-15 19:14:26,975 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
packageJobJar: [flow_statistic_mapper.py, flow_statistic_reducer.py, /tmp/hadoop-unjar3114046136813781093/] [] /tmp/streamjob6407868495582297159.jar tmpDir=null
2018-05-15 19:14:28,667 INFO client.RMProxy: Connecting to ResourceManager at node-master/120.77.239.67:18040
2018-05-15 19:14:28,944 INFO client.RMProxy: Connecting to ResourceManager at node-master/120.77.239.67:18040
2018-05-15 19:14:29,587 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1526300938491_0016
2018-05-15 19:14:30,598 INFO mapred.FileInputFormat: Total input files to process : 24
2018-05-15 19:14:30,741 INFO mapreduce.JobSubmitter: number of splits:24
2018-05-15 19:14:30,789 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2018-05-15 19:14:31,866 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526300938491_0016
2018-05-15 19:14:31,868 INFO mapreduce.JobSubmitter: Executing with tokens: []
2018-05-15 19:14:32,071 INFO conf.Configuration: resource-types.xml not found
2018-05-15 19:14:32,072 INFO resource.ResourceUtils: Unable to find &#39;resource-types.xml&#39;.
2018-05-15 19:14:32,177 INFO impl.YarnClientImpl: Submitted application application_1526300938491_0016
2018-05-15 19:14:32,229 INFO mapreduce.Job: The url to track the job: http://node-master:18088/proxy/application_1526300938491_0016/
2018-05-15 19:14:32,231 INFO mapreduce.Job: Running job: job_1526300938491_0016
2018-05-15 19:14:38,323 INFO mapreduce.Job: Job job_1526300938491_0016 running in uber mode : false
2018-05-15 19:14:38,325 INFO mapreduce.Job:  map 0% reduce 0%
2018-05-15 19:14:46,398 INFO mapreduce.Job:  map 8% reduce 0%
2018-05-15 19:14:50,419 INFO mapreduce.Job:  map 21% reduce 0%
2018-05-15 19:14:54,438 INFO mapreduce.Job:  map 25% reduce 0%
2018-05-15 19:14:56,449 INFO mapreduce.Job:  map 29% reduce 0%
2018-05-15 19:15:04,487 INFO mapreduce.Job:  map 38% reduce 0%
2018-05-15 19:15:05,492 INFO mapreduce.Job:  map 42% reduce 0%
2018-05-15 19:15:06,497 INFO mapreduce.Job:  map 50% reduce 0%
2018-05-15 19:15:14,534 INFO mapreduce.Job:  map 54% reduce 0%
2018-05-15 19:15:15,539 INFO mapreduce.Job:  map 58% reduce 0%
2018-05-15 19:15:21,569 INFO mapreduce.Job:  map 67% reduce 0%
2018-05-15 19:15:23,578 INFO mapreduce.Job:  map 71% reduce 0%
2018-05-15 19:15:24,582 INFO mapreduce.Job:  map 75% reduce 0%
2018-05-15 19:15:30,608 INFO mapreduce.Job:  map 75% reduce 25%
2018-05-15 19:15:31,613 INFO mapreduce.Job:  map 79% reduce 25%
2018-05-15 19:15:32,617 INFO mapreduce.Job:  map 88% reduce 25%
2018-05-15 19:15:34,626 INFO mapreduce.Job:  map 92% reduce 25%
2018-05-15 19:15:36,634 INFO mapreduce.Job:  map 92% reduce 31%
2018-05-15 19:15:39,646 INFO mapreduce.Job:  map 96% reduce 31%
2018-05-15 19:15:40,651 INFO mapreduce.Job:  map 100% reduce 31%
2018-05-15 19:15:41,659 INFO mapreduce.Job:  map 100% reduce 100%
2018-05-15 19:15:43,676 INFO mapreduce.Job: Job job_1526300938491_0016 completed successfully
2018-05-15 19:15:43,784 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=2208548
		FILE: Number of bytes written=9857943
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=864242
		HDFS: Number of bytes written=303
		HDFS: Number of read operations=77
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=24
		Launched reduce tasks=1
		Data-local map tasks=24
		Total time spent by all maps in occupied slots (ms)=167511
		Total time spent by all reduces in occupied slots (ms)=32319
		Total time spent by all map tasks (ms)=167511
		Total time spent by all reduce tasks (ms)=32319
		Total vcore-milliseconds taken by all map tasks=167511
		Total vcore-milliseconds taken by all reduce tasks=32319
		Total megabyte-milliseconds taken by all map tasks=343062528
		Total megabyte-milliseconds taken by all reduce tasks=66189312
	Map-Reduce Framework
		Map input records=87876
		Map output records=35060
		Map output bytes=2138422
		Map output materialized bytes=2208686
		Input split bytes=3864
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=2208686
		Reduce input records=35060
		Reduce output records=9
		Spilled Records=70120
		Shuffled Maps =24
		Failed Shuffles=0
		Merged Map outputs=24
		GC time elapsed (ms)=3650
		CPU time spent (ms)=23560
		Physical memory (bytes) snapshot=8264720384
		Virtual memory (bytes) snapshot=66202730496
		Total committed heap usage (bytes)=6004146176
		Peak Map Physical memory (bytes)=346320896
		Peak Map Virtual memory (bytes)=2619580416
		Peak Reduce Physical memory (bytes)=210169856
		Peak Reduce Virtual memory (bytes)=3486892032
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=860378
	File Output Format Counters 
		Bytes Written=303
</code></pre></td></tr></table>
</div>
</div><h1 id="流量数据导出到mysql">流量数据导出到Mysql</h1>
<p>使用Sqoop导出HDFS中的流量数据到Mysql中，需要创建有对应字段的新表，具体使用参见Sqoop导入导出文档。</p>
<h1 id="hadoop-streaming错误排查">hadoop streaming错误排查</h1>
<p>使用hadoop streaming编写MR程序时最常见的错误：<strong>hadoop-streaming-subprocess-failed-with-code-1</strong></p>
<p>对应的需要检查以下几个问题：</p>
<ol>
<li>如果是通过./mapper.py的方式执行，则需要给mapper.py增加执行权限</li>
<li>python shell命令执行时，py文件头部需要指定<code>#!/usr/bin/env python</code></li>
<li>Python环境和程序依赖的第三方库需要在集群中的所有节点上安装</li>
</ol>
<p>上述几项没有问题之后，基本就是代码层面的问题了。需要逐层排查</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://www.cnblogs.com/joyeecheung/p/3841952.html">用python + hadoop streaming 编写分布式程序（三） &ndash; 自定义功能</a></li>
<li><a href="https://www.cnblogs.com/joyeecheung/p/3757915.html">用python + hadoop streaming 编写分布式程序（一） &ndash; 原理介绍，样例程序与本地调试</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.0.0/hadoop-streaming/HadoopStreaming.html">官方-Hadoop Streaming</a></li>
<li><a href="http://grokbase.com/t/cloudera/cdh-user/132he822ep/hadoop-streaming-subprocess-failed-with-code-1">问题排查-Hadoop streaming - Subprocess failed with code 1</a></li>
<li><a href="http://chenlly.com/2017/04/18/Hadoop-Python%E5%AE%9E%E7%8E%B0HadoopStreaming%E5%88%86%E7%BB%84%E5%92%8C%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/">Hadoop-Python实现Hadoop Streaming分组和二次排序</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-compression-analysis/index.html">IBM-Hadoop 压缩实现分析</a></li>
<li><a href="http://blog.51cto.com/balich/2067858">hadoop mapreduce开发实践之HDFS压缩文件（-cacheArchive）</a></li>
<li><a href="http://icejoywoo.github.io/2015/09/28/introduction-to-hadoop-streaming.html">Hadoop Streaming入门</a></li>
<li><a href="http://blog.51cto.com/xpleaf/2095836">大数据采集、清洗、处理：使用MapReduce进行离线数据分析完整案例</a></li>
<li><a href="https://blog.csdn.net/bitcarmanlee/article/details/51735053">hadoop 代码中获取文件名 </a></li>
</ul>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">Suncle</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2018-05-16
        
    </span>
  </p>
  
  
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://flowsnow.oss-cn-shanghai.aliyuncs.com/history/wechat-reward-image.png">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://flowsnow.oss-cn-shanghai.aliyuncs.com/history/alipay-reward-image.jpg">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E9%A1%B9%E7%9B%AE/">项目</a>
          <a href="/tags/Hadoop/">Hadoop</a>
          <a href="/tags/%E7%BB%9F%E8%AE%A1/">统计</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2018/05/17/Classic-Interview-Questions/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">经典面试题</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/2018/05/09/Sqoop-Import-Export/">
            <span class="next-text nav-default">Sqoop导入导出</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="suncle1993/suncle1993.github.io"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:im.suncle@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://www.facebook.com/sunclechen" class="iconfont icon-facebook" title="facebook"></a>
      <a href="https://github.com/suncle1993" class="iconfont icon-github" title="github"></a>
      <a href="https://weibo.com/3655576503" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://www.zhihu.com/people/flowsnow" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://space.bilibili.com/362765899" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://suncle.me/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2015 - 
    2020<span class="heart"><i class="iconfont icon-heart"></i></span><span>Suncle</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-72506112-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script id="baidu_analytics">
  var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?41fc030db57d5570dd22f78997dc4a7e";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
