<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>搭建Hadoop3集群 - Suncle&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Suncle" /><meta name="description" content="强烈建议再搭建hadoop集群之前体验一下单机模式和伪分布式模式的搭建过程，可以参考以下链接：
 https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/  开始之前 本次集群搭建所依赖的软件环境如下：
 Linux系统：以运行在阿里云ECS上的Ubuntu 16.04 LTS版本为例 jdk-8u162-linux-x64.tar.gz hadoop 3.1.0  先了解一个概念：
 Hadoop YARN： YARN是一个在所有节点上执行数据处理任务的作业调度框架。
 然后执行以下初始步骤：
 创建三台阿里云ECS，也可以在本地创建3台配置较好的Vmware虚拟机。分别作为hadoop集群的node-master，node1和node2（名称可以自取）。 建议将每个主机名设置为节点名 ，一定要修改hostname。 为每台机器创建hadoop用户，后续如没有特殊说明，所有命令均在hadoop用户下执行。 在三台机器上都安装jdk，统一使用hadoop用户安装在/usr/local/src目录下（其他目录也可，放在用户目录下会更好，省掉权限问题），更改/usr/local/src目录的属主和属组为hadoop，可以使用chown hadoop:hadoop /usr/local/src命令更改。 需要在各个节点的/bin目录下增加java可执行文件的软连接，以node2为例  1 2  hadoop@node2:~$ cd /bin hadoop@node2:/bin$ sudo ln -s /usr/local/src/jdk1.8.0_162/bin/java java   如果没有添加，在执行MR程序时会报错：/bin/bash: /bin/java: No such file or directory
创建hadoop用户和安装jdk的步骤参见文章开头的单机和伪分布式搭建过程。
下面是本次集群安装的三台ECS机器的ip情况：
 node-master: 120.77.239.67 node1: 119.23.145.73 node2: 119.23.141.223 " /><meta name="keywords" content="计算机, 后端, Python, golang" />






<meta name="generator" content="Hugo 0.74.3 with theme even" />


<link rel="canonical" href="https://suncle.me/2018/05/03/Building-Hadoop3-Cluster/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link href="/sass/main.min.651e6917abb0239242daa570c2bec9867267bbcd83646da5a850afe573347b44.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="搭建Hadoop3集群" />
<meta property="og:description" content="强烈建议再搭建hadoop集群之前体验一下单机模式和伪分布式模式的搭建过程，可以参考以下链接：

https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/

开始之前
本次集群搭建所依赖的软件环境如下：

Linux系统：以运行在阿里云ECS上的Ubuntu 16.04 LTS版本为例
jdk-8u162-linux-x64.tar.gz
hadoop 3.1.0

先了解一个概念：

Hadoop YARN： YARN是一个在所有节点上执行数据处理任务的作业调度框架。

然后执行以下初始步骤：

创建三台阿里云ECS，也可以在本地创建3台配置较好的Vmware虚拟机。分别作为hadoop集群的node-master，node1和node2（名称可以自取）。 建议将每个主机名设置为节点名 ，一定要修改hostname。
为每台机器创建hadoop用户，后续如没有特殊说明，所有命令均在hadoop用户下执行。
在三台机器上都安装jdk，统一使用hadoop用户安装在/usr/local/src目录下（其他目录也可，放在用户目录下会更好，省掉权限问题），更改/usr/local/src目录的属主和属组为hadoop，可以使用chown hadoop:hadoop /usr/local/src命令更改。
需要在各个节点的/bin目录下增加java可执行文件的软连接，以node2为例



1
2


hadoop@node2:~$ cd /bin
hadoop@node2:/bin$ sudo ln -s /usr/local/src/jdk1.8.0_162/bin/java java


如果没有添加，在执行MR程序时会报错：/bin/bash: /bin/java: No such file or directory
创建hadoop用户和安装jdk的步骤参见文章开头的单机和伪分布式搭建过程。
下面是本次集群安装的三台ECS机器的ip情况：

node-master: 120.77.239.67
node1: 119.23.145.73
node2: 119.23.141.223
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://suncle.me/2018/05/03/Building-Hadoop3-Cluster/" />
<meta property="article:published_time" content="2018-05-03T16:54:05+00:00" />
<meta property="article:modified_time" content="2018-05-03T16:54:05+00:00" />
<meta itemprop="name" content="搭建Hadoop3集群">
<meta itemprop="description" content="强烈建议再搭建hadoop集群之前体验一下单机模式和伪分布式模式的搭建过程，可以参考以下链接：

https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/

开始之前
本次集群搭建所依赖的软件环境如下：

Linux系统：以运行在阿里云ECS上的Ubuntu 16.04 LTS版本为例
jdk-8u162-linux-x64.tar.gz
hadoop 3.1.0

先了解一个概念：

Hadoop YARN： YARN是一个在所有节点上执行数据处理任务的作业调度框架。

然后执行以下初始步骤：

创建三台阿里云ECS，也可以在本地创建3台配置较好的Vmware虚拟机。分别作为hadoop集群的node-master，node1和node2（名称可以自取）。 建议将每个主机名设置为节点名 ，一定要修改hostname。
为每台机器创建hadoop用户，后续如没有特殊说明，所有命令均在hadoop用户下执行。
在三台机器上都安装jdk，统一使用hadoop用户安装在/usr/local/src目录下（其他目录也可，放在用户目录下会更好，省掉权限问题），更改/usr/local/src目录的属主和属组为hadoop，可以使用chown hadoop:hadoop /usr/local/src命令更改。
需要在各个节点的/bin目录下增加java可执行文件的软连接，以node2为例



1
2


hadoop@node2:~$ cd /bin
hadoop@node2:/bin$ sudo ln -s /usr/local/src/jdk1.8.0_162/bin/java java


如果没有添加，在执行MR程序时会报错：/bin/bash: /bin/java: No such file or directory
创建hadoop用户和安装jdk的步骤参见文章开头的单机和伪分布式搭建过程。
下面是本次集群安装的三台ECS机器的ip情况：

node-master: 120.77.239.67
node1: 119.23.145.73
node2: 119.23.141.223
">
<meta itemprop="datePublished" content="2018-05-03T16:54:05+00:00" />
<meta itemprop="dateModified" content="2018-05-03T16:54:05+00:00" />
<meta itemprop="wordCount" content="5575">



<meta itemprop="keywords" content="集群,搭建,YARN,Hadoop," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="搭建Hadoop3集群"/>
<meta name="twitter:description" content="强烈建议再搭建hadoop集群之前体验一下单机模式和伪分布式模式的搭建过程，可以参考以下链接：

https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/

开始之前
本次集群搭建所依赖的软件环境如下：

Linux系统：以运行在阿里云ECS上的Ubuntu 16.04 LTS版本为例
jdk-8u162-linux-x64.tar.gz
hadoop 3.1.0

先了解一个概念：

Hadoop YARN： YARN是一个在所有节点上执行数据处理任务的作业调度框架。

然后执行以下初始步骤：

创建三台阿里云ECS，也可以在本地创建3台配置较好的Vmware虚拟机。分别作为hadoop集群的node-master，node1和node2（名称可以自取）。 建议将每个主机名设置为节点名 ，一定要修改hostname。
为每台机器创建hadoop用户，后续如没有特殊说明，所有命令均在hadoop用户下执行。
在三台机器上都安装jdk，统一使用hadoop用户安装在/usr/local/src目录下（其他目录也可，放在用户目录下会更好，省掉权限问题），更改/usr/local/src目录的属主和属组为hadoop，可以使用chown hadoop:hadoop /usr/local/src命令更改。
需要在各个节点的/bin目录下增加java可执行文件的软连接，以node2为例



1
2


hadoop@node2:~$ cd /bin
hadoop@node2:/bin$ sudo ln -s /usr/local/src/jdk1.8.0_162/bin/java java


如果没有添加，在执行MR程序时会报错：/bin/bash: /bin/java: No such file or directory
创建hadoop用户和安装jdk的步骤参见文章开头的单机和伪分布式搭建过程。
下面是本次集群安装的三台ECS机器的ip情况：

node-master: 120.77.239.67
node1: 119.23.145.73
node2: 119.23.141.223
"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Suncle&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Suncle&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">搭建Hadoop3集群</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-05-03 </span>
        <div class="post-category">
            <a href="/categories/Hadoop/"> Hadoop </a>
            </div>
          <span class="more-meta"> 约 5575 字 </span>
          <span class="more-meta"> 预计阅读 12 分钟 </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#开始之前">开始之前</a></li>
    <li><a href="#hadoop集群架构">Hadoop集群架构</a></li>
    <li><a href="#配置集群">配置集群</a>
      <ul>
        <li><a href="#在每个节点上创建主机文件">在每个节点上创建主机文件</a></li>
        <li><a href="#修改所有节点hostname文件">修改所有节点hostname文件</a></li>
        <li><a href="#为hadoop用户分配认证密钥对">为Hadoop用户分配认证密钥对</a></li>
        <li><a href="#下载hadoop安装包并上传">下载hadoop安装包并上传</a></li>
        <li><a href="#设置hadoop环境变量">设置hadoop环境变量</a></li>
      </ul>
    </li>
    <li><a href="#配置管理节点">配置管理节点</a>
      <ul>
        <li><a href="#设置hadoop依赖的java环境变量">设置hadoop依赖的java环境变量</a></li>
        <li><a href="#配置core-sitexml">配置core-site.xml</a></li>
        <li><a href="#配置hdfs-sitexml">配置hdfs-site.xml</a></li>
        <li><a href="#配置mapred-sitexml">配置mapred-site.xml</a></li>
        <li><a href="#配置yarn-sitexml文件">配置yarn-site.xml文件</a></li>
        <li><a href="#配置workers文件">配置workers文件</a></li>
      </ul>
    </li>
    <li><a href="#配置内存分配">配置内存分配</a>
      <ul>
        <li><a href="#内存分配属性">内存分配属性</a></li>
        <li><a href="#各内存大小计算方式">各内存大小计算方式</a></li>
        <li><a href="#8gb节点的示例配置">8GB节点的示例配置</a></li>
      </ul>
    </li>
    <li><a href="#配置从节点">配置从节点</a></li>
    <li><a href="#格式化hdfs">格式化HDFS</a></li>
    <li><a href="#启动停止hdfs">启动停止HDFS</a></li>
    <li><a href="#运行yarn">运行YARN</a>
      <ul>
        <li><a href="#启动停止yarn">启动停止YARN</a></li>
        <li><a href="#监控yarn">监控YARN</a></li>
        <li><a href="#提交mapreduce作业至yarn">提交MapReduce作业至YARN</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>强烈建议再搭建hadoop集群之前体验一下单机模式和伪分布式模式的搭建过程，可以参考以下链接：</p>
<ul>
<li><a href="https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/">https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/</a></li>
</ul>
<h1 id="开始之前">开始之前</h1>
<p>本次集群搭建所依赖的软件环境如下：</p>
<ol>
<li>Linux系统：以运行在阿里云ECS上的Ubuntu 16.04 LTS版本为例</li>
<li>jdk-8u162-linux-x64.tar.gz</li>
<li>hadoop 3.1.0</li>
</ol>
<p>先了解一个概念：</p>
<blockquote>
<p><strong>Hadoop YARN</strong>： YARN是一个在所有节点上执行数据处理任务的作业调度框架。</p>
</blockquote>
<p>然后执行以下初始步骤：</p>
<ol>
<li>创建三台阿里云ECS，也可以在本地创建3台配置较好的Vmware虚拟机。分别作为hadoop集群的node-master，node1和node2（名称可以自取）。 <del>建议将每个主机名设置为节点名</del> ，一定要修改hostname。</li>
<li>为每台机器创建hadoop用户，后续如没有特殊说明，所有命令均在hadoop用户下执行。</li>
<li>在三台机器上都安装jdk，统一使用hadoop用户安装在<code>/usr/local/src</code>目录下（其他目录也可，放在用户目录下会更好，省掉权限问题），更改<code>/usr/local/src</code>目录的属主和属组为hadoop，可以使用<code>chown hadoop:hadoop /usr/local/src</code>命令更改。</li>
<li>需要在各个节点的/bin目录下增加java可执行文件的软连接，以node2为例</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">hadoop@node2:~$ <span class="nb">cd</span> /bin
hadoop@node2:/bin$ sudo ln -s /usr/local/src/jdk1.8.0_162/bin/java java
</code></pre></td></tr></table>
</div>
</div><p>如果没有添加，在执行MR程序时会报错：<code>/bin/bash: /bin/java: No such file or directory</code></p>
<p>创建hadoop用户和安装jdk的步骤参见文章开头的单机和伪分布式搭建过程。</p>
<p>下面是本次集群安装的三台ECS机器的ip情况：</p>
<ul>
<li><strong>node-master</strong>: 120.77.239.67</li>
<li><strong>node1</strong>: 119.23.145.73</li>
<li><strong>node2</strong>: 119.23.141.223</li>
</ul>
<h1 id="hadoop集群架构">Hadoop集群架构</h1>
<p>在配置主从节点之前，了解Hadoop集群的不同组件是非常重要的。</p>
<p>主节点保存有关分布式文件系统的信息，例如ext3文件系统上的inode表，并调度资源分配。 此次搭建过程中node-master即为主节点，并运行两个守护进程：</p>
<ul>
<li><strong>NameNode</strong>：管理分布式文件系统并知道集群内存储的数据块的位置。</li>
<li><strong>ResourceManager</strong>：管理YARN作业，监管从节点上的调度进程和执行进程。</li>
</ul>
<p>从节点存储实际数据并提供处理能力来运行作业。分别是node1和node2，并运行两个守护进程：</p>
<ul>
<li><strong>DataNode</strong>：管理物理存储在节点上的实际数据。</li>
<li><strong>NodeManager</strong>：管理节点上任务的执行。</li>
</ul>
<h1 id="配置集群">配置集群</h1>
<h2 id="在每个节点上创建主机文件">在每个节点上创建主机文件</h2>
<p>要想使用节点名称通信，需要编辑<code>/etc/hosts</code>文件以添加三台服务器的IP地址。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">120.77.239.67     node-master
119.23.145.73     node1
119.23.141.223    node2
</code></pre></td></tr></table>
</div>
</div><p>相当于给ip取名称。</p>
<h2 id="修改所有节点hostname文件">修改所有节点hostname文件</h2>
<p><strong>这一步骤一定要操作</strong>：以管理节点为例进行操作</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">sudo vim /etc/hostname
</code></pre></td></tr></table>
</div>
</div><p>替换掉其中已有的<code>hostname</code>，写入<code>node-master</code>，和上述hosts文件中保持一致即可。</p>
<p>如果这个步骤不修改则会在后续集群中执行MapReduce程序过程中出现以下错误：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">2018-05-08 19:50:46,481 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: Error trying to assign container token and NM token to an updated container container_1525778560515_0005_01_000001
java.lang.IllegalArgumentException: java.net.UnknownHostException: iZwz99xn3877js1s191xp9Z
        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:445)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: iZwz99xn3877js1s191xp9Z
</code></pre></td></tr></table>
</div>
</div><p>意思是管理节点无法识别从节点的hostname，因为在管理节点的hosts文件中对应的是node2，而不是node2的真是hosname，也就是iZwz99xn3877js1s191xp9Z。因此一定要修改hostname。</p>
<p>参考：http://www.voidcn.com/article/p-dsepxqfl-pz.html</p>
<h2 id="为hadoop用户分配认证密钥对">为Hadoop用户分配认证密钥对</h2>
<p>主节点将使用ssh协议通过密钥对认证连接到其他节点，以管理群集。</p>
<p>以hadoop用户身份登录到node-master，并生成一个ssh-key（如果执行已生成过ssh-key则会提示重复，是否需要重写，此时忽略即可）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">ssh-keygen -b <span class="m">4096</span>
</code></pre></td></tr></table>
</div>
</div><p>将密钥复制到其他节点。 将密钥复制到节点主机本身也是一种很好的做法，这样您可以根据需要将它用作DataNode。 输入以下命令，并在询问时输入hadoop用户的密码。 如果提示是否将密钥添加到已知主机，请输入yes：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@node-master
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@node1
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@node2
</code></pre></td></tr></table>
</div>
</div><h2 id="下载hadoop安装包并上传">下载hadoop安装包并上传</h2>
<p>以hadoop用户身份登录到node-master，将下载好的安装包上传并解压：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /usr/local/src
tar -xzvf jdk-8u162-linux-x64.tar.gz
</code></pre></td></tr></table>
</div>
</div><h2 id="设置hadoop环境变量">设置hadoop环境变量</h2>
<p>编辑<code>~/.profile</code>文件并写入以下内容：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># hadoop install env settings</span>
<span class="nv">HADOOP_INSTALL</span><span class="o">=</span>/usr/local/src/hadoop-3.1.0
<span class="nv">PATH</span><span class="o">=</span><span class="nv">$HADOOP_INSTALL</span>/bin:<span class="nv">$HADOOP_INSTALL</span>/sbin:<span class="nv">$PATH</span>
<span class="nb">export</span> HADOOP_INSTALL PATH
</code></pre></td></tr></table>
</div>
</div><h1 id="配置管理节点">配置管理节点</h1>
<p>配置将在node-master上完成并复制到其他节点。</p>
<h2 id="设置hadoop依赖的java环境变量">设置hadoop依赖的java环境变量</h2>
<p>修改<code>/usr/local/src/hadoop-3.1.0/etc/hadoop/hadoop-env.sh</code>文件中的<code>JAVA_HOME</code>变量，改为实际的即可：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># Many of the options here are built from the perspective that users</span>
<span class="c1"># may want to provide OVERWRITING values on the command line.</span>
<span class="c1"># For example:</span>
#
<span class="c1">#  JAVA_HOME=/usr/java/testing hdfs dfs -ls</span>
<span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/local/src/jdk1.8.0_162
#
<span class="c1"># Therefore, the vast majority (BUT NOT ALL!) of these defaults</span>
<span class="c1"># are configured for substitution and not append.  If append</span>
<span class="c1"># is preferable, modify this file accordingly.</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="配置core-sitexml">配置core-site.xml</h2>
<p>在master主机上配置hdfs地址，注意和伪分布式的略微不同，需要直接指定master节点所在的地址。在<code>/usr/local/src/hadoop-3.1.0/etc/hadoop/core-site.xml</code>文件中写入以下内容：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>file:/usr/local/src/hadoop-3.1.0/tmp<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;description&gt;</span>Abase for other temporary directories.<span class="nt">&lt;/description&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>hdfs://node-master:9000<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="配置hdfs-sitexml">配置hdfs-site.xml</h2>
<p>配置副本的个数及数据的存放路径，在<code>/usr/local/src/hadoop-3.1.0/etc/hadoop/hdfs-site.xml</code>文件中写入：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>file:/usr/local/src/hadoop-3.1.0/tmp/dfs/name<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>file:/usr/local/src/hadoop-3.1.0/tmp/dfs/data<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></td></tr></table>
</div>
</div><p>其中：</p>
<ul>
<li><code>dfs.replication</code> 表示数据块的副本数量，指示数据在集群中的复制次数。 您可以设置2以将所有数据复制到两个节点上。 不要设置高于实际节点数量的值。</li>
<li><code>dfs.namenode.name.dir</code> 元数据存放路径</li>
<li><code>dfs.datanode.data.dir</code> 数据节点存放路径</li>
</ul>
<h2 id="配置mapred-sitexml">配置mapred-site.xml</h2>
<p>设置YARN为作业调度器，也就是默认的MapReduce框架，在<code>/usr/local/src/hadoop-3.1.0/etc/hadoop/mapred-site.xml</code>文件中写入：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="配置yarn-sitexml文件">配置yarn-site.xml文件</h2>
<p>在<code>/usr/local/src/hadoop-3.1.0/etc/hadoop/yarn-site.xml</code>文件中写入：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
<span class="c">&lt;!-- Site specific YARN configuration properties --&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>node-master:18040<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.scheduler.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>node-master:18030<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.webapp.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>node-master:18088<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>node-master:18025<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.admin.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>node-master:18141<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.app.mapreduce.am.env<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>HADOOP_MAPRED_HOME=/usr/local/src/hadoop-3.1.0<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>mapreduce.map.env<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>HADOOP_MAPRED_HOME=/usr/local/src/hadoop-3.1.0<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>mapreduce.reduce.env<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>HADOOP_MAPRED_HOME=/usr/local/src/hadoop-3.1.0<span class="nt">&lt;/value&gt;</span>
  <span class="err">&lt;</span>/property
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></td></tr></table>
</div>
</div><p>注意修改的各个<code>value</code>需要和<code>/etc/hosts</code>中的名称保持一致。</p>
<p>这三项配置一定要有：<code>yarn.app.mapreduce.am.env</code> <code>mapreduce.map.env</code> <code>mapreduce.reduce.env</code>否则在执行MR程序时会直接报错（hadoop3.1中已验证）。</p>
<p>具体错误参考：https://stackoverflow.com/questions/47599789/hadoop-pagerank-error-when-running</p>
<h2 id="配置workers文件">配置workers文件</h2>
<p>列出所有workers的主机名。在<code>/usr/local/src/hadoop-3.1.0/etc/hadoop/workers</code>文件中写入：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">node-master
node1
node2
</code></pre></td></tr></table>
</div>
</div><p>注意：</p>
<ol>
<li>hadoop2.x配置的是slaves文件，这里有所改变。</li>
<li>此处的worker中写入了管理节点，因此启动HDFS之后也会在管理节点所在机器创建一个DataNode。如果不想在管理节点机器中开启DataNode，则删除workers文件中的node-master配置。</li>
</ol>
<p>此外，如果想在Hadoop集群中动态增加和删除节点，则更改此文件即可。</p>
<h1 id="配置内存分配">配置内存分配</h1>
<p>内存分配在低RAM节点上可能会很棘手，因为默认值不适用于RAM少于8GB的节点，因此在使用sqoop等命令时调用的MapReduce程序会有如下类似的报错：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">Application is added to the scheduler and is not yet activated. Queue&#39;s AM resource limit exceeded. Details : AM Partition = &lt;DEFAULT_PARTITION&gt;; AM Resource Request = &lt;memory:2048, vCores:1&gt;; Queue Resource Limit for AM = &lt;memory:3072, vCores:1&gt;; User AM Resource Limit of the queue = &lt;memory:3072, vCores:1&gt;; Queue AM Resource Usage = &lt;memory:2048, vCores:1&gt;;
</code></pre></td></tr></table>
</div>
</div><p>这里将重点介绍如何为MapReduce作业分配内存，因为此次使用的ECS机器是4GB内存，因此为4GB RAM节点提供示例配置。</p>
<h2 id="内存分配属性">内存分配属性</h2>
<p>YARN作业执行需要使用以下两种资源：</p>
<ul>
<li>Application Master (AM) ：负责监视应用程序并协调集群中的分布式执行程序。</li>
<li>Executors：一些由AM创建的Executors，用于真正的运行该作业。 对于MapReduce作业，executors会并行的执行map和reduce操作。</li>
</ul>
<p>两者都在从节点的容器中运行。 每个从节点都运行一个NodeManager守护进程，负责在节点上创建容器。 整个集群由一个ResourceManager管理，它根据容量要求和当前使用情况调度所有所有从节点上的容器分配。</p>
<p>需要正确配置四种类型的资源分配才能使群集正常工作。分别是：</p>
<ol>
<li>可以为单个节点上的YARN容器分配的内存大小。 这个限制应该高于其他所有的限制; 否则，容器分配会被拒绝，应用程序失败。 但是，它不应该是节点上的全部RAM。</li>
</ol>
<blockquote>
<p>这个值在<code>yarn-site.xml</code>中配置<code>yarn.nodemanager.resource.memory-mb</code>属性</p>
</blockquote>
<ol start="2">
<li>单个容器可以消耗的内存大小以及允许的最小内存分配量。 一个容器永远不会超过最大容量，否则分配将失败，并且总是以最小分配量的倍数进行RAM分配。</li>
</ol>
<blockquote>
<p>这些值在<code>yarn-site.xml</code>中配置<code>yarn.scheduler.maximum-allocation-mb</code>和<code>yarn.scheduler.minimum-allocation-mb</code>属性。</p>
</blockquote>
<ol start="3">
<li>分配给ApplicationMaster的内存大小。 是一个适合容器最大尺寸的常数值。</li>
</ol>
<blockquote>
<p>这个值在<code>mapred-site.xml</code>中配置<code>yarn.app.mapreduce.am.resource.mb</code>属性。</p>
</blockquote>
<ol start="4">
<li>分配给map和reduce操作的内存大小。应该小于最大尺寸。</li>
</ol>
<blockquote>
<p>这是在<code>mapred-site.xml</code>中配置的，其属性为<code>mapreduce.map.memory.mb</code>和<code>mapreduce.reduce.memory.mb</code>。</p>
</blockquote>
<p>具体配置参数可以参见：https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html</p>
<h2 id="各内存大小计算方式">各内存大小计算方式</h2>
<p>下载内存计算脚本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">wget https://raw.githubusercontent.com/mahadevkonar/ambari-yarn-utils/master/yarn-utils/yarn-utils.py
</code></pre></td></tr></table>
</div>
</div><p>使用方法：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">python yarn-utils.py -c 16 -m 64 -d 4 -k True
</code></pre></td></tr></table>
</div>
</div><ul>
<li>-c选项：cpu核数</li>
<li>-m选项：内存大小</li>
<li>-d选项：机器上的磁盘数量</li>
<li>-k选项：如果安装了HBase则设置为True，否则为False</li>
</ul>
<blockquote>
<p>其中：Core的数量可以通过<code>nproc</code>命令计算；内存大小可以通过<code>free -m</code>命令来计算需要换算为G;磁盘的数量可以通过<code>lsblk -s</code>或<code>sudo fdisk -l</code>命令来查看。</p>
</blockquote>
<p>计算完成之后，最后的脚本执行命令为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">hadoop@node-master:~$ python yarn-utils.py -c <span class="m">2</span> -m <span class="m">8</span> -d <span class="m">1</span> -k False
 Using <span class="nv">cores</span><span class="o">=</span><span class="m">2</span> <span class="nv">memory</span><span class="o">=</span>8GB <span class="nv">disks</span><span class="o">=</span><span class="m">1</span> <span class="nv">hbase</span><span class="o">=</span>False
 Profile: <span class="nv">cores</span><span class="o">=</span><span class="m">2</span> <span class="nv">memory</span><span class="o">=</span>6144MB <span class="nv">reserved</span><span class="o">=</span>2GB <span class="nv">usableMem</span><span class="o">=</span>6GB <span class="nv">disks</span><span class="o">=</span><span class="m">1</span>
 Num <span class="nv">Container</span><span class="o">=</span><span class="m">3</span>
 Container <span class="nv">Ram</span><span class="o">=</span>2048MB
 Used <span class="nv">Ram</span><span class="o">=</span>6GB
 Unused <span class="nv">Ram</span><span class="o">=</span>2GB
 yarn.scheduler.minimum-allocation-mb<span class="o">=</span><span class="m">2048</span>
 yarn.scheduler.maximum-allocation-mb<span class="o">=</span><span class="m">6144</span>
 yarn.nodemanager.resource.memory-mb<span class="o">=</span><span class="m">6144</span>
 mapreduce.map.memory.mb<span class="o">=</span><span class="m">1024</span>
 mapreduce.map.java.opts<span class="o">=</span>-Xmx819m
 mapreduce.reduce.memory.mb<span class="o">=</span><span class="m">2048</span>
 mapreduce.reduce.java.opts<span class="o">=</span>-Xmx1638m
 yarn.app.mapreduce.am.resource.mb<span class="o">=</span><span class="m">1024</span>
 yarn.app.mapreduce.am.command-opts<span class="o">=</span>-Xmx819m
 mapreduce.task.io.sort.mb<span class="o">=</span><span class="m">409</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="8gb节点的示例配置">8GB节点的示例配置</h2>
<table>
<thead>
<tr>
<th>属性</th>
<th>值</th>
</tr>
</thead>
<tbody>
<tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>6144</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>6144</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>2048</td>
</tr>
<tr>
<td>yarn.app.mapreduce.am.resource.mb</td>
<td>1024</td>
</tr>
<tr>
<td>mapreduce.map.memory.mb</td>
<td>1024</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>2048</td>
</tr>
</tbody>
</table>
<p>编辑 <code>/usr/local/src/hadoop-3.1.0/etc/hadoop/yarn-site.xml</code> 文件，并增加以下行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-xml" data-lang="xml">  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.resource.memory-mb<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>6144<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>6144<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>2048<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
</code></pre></td></tr></table>
</div>
</div><p>编辑<code>/usr/local/src/hadoop-3.1.0/etc/hadoop/mapred-site.xml</code>文件，并增加以下行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-xml" data-lang="xml">  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>1024<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>mapreduce.map.memory.mb<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>1024<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>mapreduce.reduce.memory.mb<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>2048<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
</code></pre></td></tr></table>
</div>
</div><h1 id="配置从节点">配置从节点</h1>
<p>复制hadoop的压缩包到所有从节点（也可以使用ftp手动上传）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">scp hadoop-3.1.0.tar.gz hadoop@node1:/usr/local/src/
scp hadoop-3.1.0.tar.gz hadoop@node2:/usr/local/src/
</code></pre></td></tr></table>
</div>
</div><p>使用hadoop用户连接到所有的从节点，解压安装包：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /usr/local/src
tar -xzvf jdk-8u162-linux-x64.tar.gz
</code></pre></td></tr></table>
</div>
</div><p>复制主节点的所有hadoop配置文件到各从节点中：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">scp /usr/local/src/hadoop-3.1.0/etc/hadoop/* hadoop@node1:/usr/local/src/hadoop-3.1.0/etc/hadoop/
scp /usr/local/src/hadoop-3.1.0/etc/hadoop/* hadoop@node2:/usr/local/src/hadoop-3.1.0/etc/hadoop/
</code></pre></td></tr></table>
</div>
</div><h1 id="格式化hdfs">格式化HDFS</h1>
<p>HDFS需要像任何传统文件系统一样格式化。 在node-master上，运行以下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">hdfs namenode -format
</code></pre></td></tr></table>
</div>
</div><h1 id="启动停止hdfs">启动停止HDFS</h1>
<p>通过从node-master运行以下脚本启动HDFS：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">start-dfs.sh
</code></pre></td></tr></table>
</div>
</div><p>这个命令会启动node-master上的NameNode和SecondaryNameNode，并且根据node1和node2上的配置文件分别启动node1和node2的DataNode。</p>
<p>使用jps命令检查每个节点上的进程是否启动：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">24053 SecondaryNameNode
23721 NameNode
23850 DataNode
24205 Jps
</code></pre></td></tr></table>
</div>
</div><p>（如果node-master上也启动了一个DataNode那么在node-master上也能看到NodeManager）</p>
<p>在node1上jps结果如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">27387 Jps
27311 DataNode
</code></pre></td></tr></table>
</div>
</div><p>在node2上jps结果如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">1314 Jps
1227 DataNode
</code></pre></td></tr></table>
</div>
</div><p>要停止主节点和从节点上的HDFS，请从node-master运行以下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">stop-dfs.sh
</code></pre></td></tr></table>
</div>
</div><p>在hdfs启动之后，各种hdfs命令就都可以直接在集群上使用。</p>
<p>关于hdfs安全模式的解除：重启机器等操作时会导致hdfs处于安全模式，因此需要用命令解除：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">hdfs dfsadmin -safemode leave
</code></pre></td></tr></table>
</div>
</div><h1 id="运行yarn">运行YARN</h1>
<p>HDFS是一个分布式存储系统，它不提供任何服务来运行和调度集群中的任务。 这是YARN框架的作用。 以下部分是关于启动，监控和向YARN提交作业。</p>
<h2 id="启动停止yarn">启动停止YARN</h2>
<p>运行以下脚本启动：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">start-yarn.sh
</code></pre></td></tr></table>
</div>
</div><p>使用jps命令检查各节点上正在运行的进程。除了前面的HDFS守护进程之外，还应该在node-master上看到ResourceManager，并在node1和node2上看到NodeManager。</p>
<p>要停止YARN，请在node-master上运行以下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">stop-yarn.sh
</code></pre></td></tr></table>
</div>
</div><h2 id="监控yarn">监控YARN</h2>
<p>yarn命令提供实用的命令套件程序来管理YARN集群。 还可以使用以下命令打印正在运行的节点的报告：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yarn node -list
</code></pre></td></tr></table>
</div>
</div><p>如果运行错误，需要检查YARN的配置文件<code>hadoop/yarn-site.xml</code>是否配置错误。</p>
<p>可以使用以下命令获取正在运行的应用程序的列表：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yarn application -list
</code></pre></td></tr></table>
</div>
</div><p>要获得yarn命令的所有可用参数，请参阅<a href="https://community.hortonworks.com/content/supportkb/49544/hdfs-client-fails-with-unknownhostexception-when-h.html">Apache YARN文档</a></p>
<p>与HDFS一样，YARN提供了一个友好的Web UI，默认端口为8088。 具体端口可通过yarn-site.xml文件里面的yarn.resourcemanager.webapp.address配置。示例地址如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">http://120.77.239.67:18088/cluster
</code></pre></td></tr></table>
</div>
</div><h2 id="提交mapreduce作业至yarn">提交MapReduce作业至YARN</h2>
<p>YARN作业被打包成jar文件，并提交给YARN用命令<code>yarn jar</code>执行。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/">https://suncle.me/2018/04/16/Hadoop3-basic-installation-and-configuration/</a></li>
<li><a href="https://www.cnblogs.com/guoyuanwei/p/8583380.html">https://www.cnblogs.com/guoyuanwei/p/8583380.html</a></li>
<li><a href="https://linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster">https://linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster</a></li>
<li><a href="http://www.dajiangtai.com/community/18389.do">http://www.dajiangtai.com/community/18389.do</a></li>
<li><a href="https://www.2cto.com/net/201610/557536.html">https://www.2cto.com/net/201610/557536.html</a></li>
</ul>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">Suncle</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2018-05-03
        
    </span>
  </p>
  
  
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://flowsnow.oss-cn-shanghai.aliyuncs.com/history/wechat-reward-image.png">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://flowsnow.oss-cn-shanghai.aliyuncs.com/history/alipay-reward-image.jpg">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E9%9B%86%E7%BE%A4/">集群</a>
          <a href="/tags/%E6%90%AD%E5%BB%BA/">搭建</a>
          <a href="/tags/YARN/">YARN</a>
          <a href="/tags/Hadoop/">Hadoop</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2018/05/09/Sqoop-Import-Export/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Sqoop导入导出</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/2018/04/17/Writing-An-Hadoop-MapReduce-Program-In-Python/">
            <span class="next-text nav-default">使用Python语言写Hadoop MapReduce程序</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="suncle1993/suncle1993.github.io"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:im.suncle@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://www.facebook.com/sunclechen" class="iconfont icon-facebook" title="facebook"></a>
      <a href="https://github.com/suncle1993" class="iconfont icon-github" title="github"></a>
      <a href="https://weibo.com/3655576503" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://www.zhihu.com/people/flowsnow" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://space.bilibili.com/362765899" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://suncle.me/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2015 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Suncle</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.d7b7ada643c9c1a983026e177f141f7363b4640d619caf01d8831a6718cd44ea.js"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-72506112-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script id="baidu_analytics">
  var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?41fc030db57d5570dd22f78997dc4a7e";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
